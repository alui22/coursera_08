##Predicting Correct Weight Lifting Technique using Human Activity Recgonition Data
####Coursera: Practical Machine Learning
####December 2015
####Anthony L.


###Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). From this data, a predictive model will be built to determine whether or not the barbell lifts are being performed correctly.

###Preparing Data
The data is downloaded and loaded for analysis. The outcome data is classe which is a factor with 5 levels: A, B, C, D, E. A represents the barbell lifts being performed correctly while the rest are all variations of an incorrect form. The predictors that were selected include measures of roll, pitch, yaw, gyration, acceleration, and magnet.

```{r, cache=T, results="hide"}
#Load Packages
library(caret)
library(randomForest)
```

```{r, cache=T, eval=F}
#Set WD
setwd("C:/Users/Anthony/Documents/Coursera/08_machine/course_project")

#Download Data
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url=url1, destfile="pml-training.csv")
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url=url2, destfile="pml-testing.csv")

#Load Data
pmltrain<-read.csv("pml-training.csv")
pmltest<-read.csv("pml-testing.csv")

#Distribution of Outcomes
table(pmltrain$classe)

#Predictors
varlist <- grep("^roll|^pitch|^yaw|^gyro|^accel|^magnet|classe", names(pmltrain))
train <- pmltrain[,varlist]
```

Due to limitations in computational speed of the machine used in this project, a data partition of 10% was used to fit the model.

```{r, cache=TRUE, eval=FALSE}
#Data Partition
inTrain <- createDataPartition(y=train$classe, p=0.1, list=F)
train <- train[inTrain,]
```


###Machine Learning Algorithm
The Random Forest machine learning algorithm is selected to predict the activity quality from the monitors. The model fit with a 92% accuracy rate.

A second model was created that used a 10-fold Repeated Cross Validation in order to enhance the model fit. This model fit with a 94% accuracy rate.


```{r, cache=TRUE, eval=FALSE}
##Random Forest Algorithm
modFit <- train(classe ~ .,
                data = train,
                prox=T,
                method = "rf")

modFit
```

Random Forest 

1964 samples
  48 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 1964, 1964, 1964, 1964, 1964, 1964, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
  
   2    0.9154334  0.8929241  0.009274588  0.01171510
   
  25    0.9213679  0.9004282  0.009910273  0.01268488
  
  48    0.9121032  0.8887111  0.010840586  0.01383490

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 25. 



```{r, cache=T, eval=FALSE}
##5-fold Repeated Cross Validation
fitControl <- trainControl(
        method = "repeatedcv",
        number = 5,
        repeats = 5)

modFit1 <- train(classe ~ .,
                data = train,
                prox=T,
                method = "rf",
                trControl = fitControl)

modFit1
```

Random Forest 

1964 samples
  48 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 1767, 1768, 1768, 1768, 1767, 1767, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD  
  
   2    0.9335730  0.9159100  0.01881949   0.02384194
   
  25    0.9405419  0.9247380  0.01542891   0.01954461
  
  48    0.9345807  0.9171861  0.01570912   0.01989765

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 25. 

###Out of Sample Error and Cross Validation
The model was fit with with two types of cross validation. The first was the default used by the random forest model which is a bootstrapped resampling with 25 reptitions which returned an out of sample error of 5.6%. The second was also with the random forest model a 10-fold cross validation resampling with 10 repititons which returned a slightly lower out of sample error of 5.3%.

```{r, cache=TRUE, eval=FALSE}
modFit$finalModel
modFit1$finalModel
```

Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = ..1) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 25

        OOB estimate of  error rate: 5.6%
Confusion matrix:
    A   B   C   D   E class.error
    
A 551   4   0   3   0  0.01254480

B  18 343  14   4   1  0.09736842

C   0  16 321   6   0  0.06413994

D   2   3  16 301   0  0.06521739

E   1   5  11   6 338  0.06371191


Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = ..1) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 25

        OOB estimate of  error rate: 5.35%
Confusion matrix:
    A   B   C   D   E class.error
    
A 550   4   1   3   0  0.01433692

B  17 350  10   3   0  0.07894737

C   0  14 320   8   1  0.06705539

D   2   4  18 298   0  0.07453416

E   1   3  10   6 341  0.05540166


###Test Cases
The random forest algorithm with 10-fold 10 reptition cross validation was used to predict the outcome of 20 test cases.

```{r, cache=TRUE, eval=F}
data.frame(pmltest$user_name,predict(modFit1, pmltest))
```

  pmltest.user_name pmltest.cvtd_timestamp predict.modFit1..pmltest.
  
1              pedro       05/12/2011 14:23                         B

2             jeremy       30/11/2011 17:11                         A

3             jeremy       30/11/2011 17:11                         B

4             adelmo       02/12/2011 13:33                         A

5             eurico       28/11/2011 14:13                         A

6             jeremy       30/11/2011 17:12                         E

7             jeremy       30/11/2011 17:12                         D

8             jeremy       30/11/2011 17:11                         D

9           carlitos       05/12/2011 11:24                         A

10           charles       02/12/2011 14:57                         A

11          carlitos       05/12/2011 11:24                         B

12            jeremy       30/11/2011 17:11                         C

13            eurico       28/11/2011 14:14                         B

14            jeremy       30/11/2011 17:10                         A

15            jeremy       30/11/2011 17:12                         E

16            eurico       28/11/2011 14:15                         E

17             pedro       05/12/2011 14:22                         A

18          carlitos       05/12/2011 11:24                         B

19             pedro       05/12/2011 14:23                         A

20            eurico       28/11/2011 14:14                         B